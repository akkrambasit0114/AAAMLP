{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Basit Akram\\AppData\\Local\\Temp\\ipykernel_4604\\3923087819.py:13: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  \"weekofyear\": s.dt.weekofyear.values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a series of datatime with a frequency of 10 hours\n",
    "s = pd.date_range('2020-01-06', '2020-01-10', freq='10H').to_series()\n",
    "\n",
    "# create some features based on datatime\n",
    "features = {\n",
    "    \"dayofweek\": s.dt.dayofweek.values,\n",
    "    \"dayofyear\": s.dt.dayofyear.values,\n",
    "    \"hour\": s.dt.hour.values,\n",
    "    \"is_leap_year\": s.dt.is_leap_year.values,\n",
    "    \"quarter\": s.dt.quarter.values,\n",
    "    \"weekofyear\": s.dt.weekofyear.values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dayofweek': array([0, 0, 0, 1, 1, 2, 2, 2, 3, 3], dtype=int64), 'dayofyear': array([6, 6, 6, 7, 7, 8, 8, 8, 9, 9], dtype=int64), 'hour': array([ 0, 10, 20,  6, 16,  2, 12, 22,  8, 18], dtype=int64), 'is_leap_year': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        True]), 'quarter': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64), 'weekofyear': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    # create a bunch of features using the data column\n",
    "    df.loc[:, 'year'] = df['date'].dt.year\n",
    "    df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear\n",
    "    df.loc[:, 'month'] = df['date'].dt.month\n",
    "    df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek\n",
    "    df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # create an aggregate dictionary\n",
    "    aggs = {}\n",
    "    # for aggregation by month, we calculate the \n",
    "    # number of unique month values and also the mean\n",
    "    aggs['month'] = ['nunique', 'mean']\n",
    "    aggs['weekofyear'] = ['nunique', 'mean']\n",
    "    # we aggregate by num1 and calculate sum, max, min\n",
    "    # and mean values of this column\n",
    "    aggs['num1'] = ['sum', 'max', 'min', 'mean']\n",
    "    # for customer_id, we calculate the total count\n",
    "    aggs['customer_id'] = ['size']\n",
    "    # again for customer_id, we calculate the total unique\n",
    "    aggs['customer_id'] = ['nunique']\n",
    "\n",
    "    # we group by customer_id and calculate the aggregates\n",
    "    agg_df = df.groupby('customer_id').agg(aggs)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 0\n",
    "\n",
    "feature_dict = {}\n",
    "\n",
    "# calculate mean\n",
    "feature_dict['mean'] = np.mean(x)\n",
    "\n",
    "# calculate max\n",
    "feature_dict['max'] = np.max(x)\n",
    "\n",
    "# calculate min \n",
    "feature_dict['min'] = np.min(x)\n",
    "\n",
    "# calculate standard dviation \n",
    "feature_dict['std'] = np.std(x)\n",
    "\n",
    "# calculate variance \n",
    "feature_dict['var'] = np.var(x)\n",
    "\n",
    "# peak-to-peak \n",
    "feature_dict['ptp'] = np.ptp(x)\n",
    "\n",
    "# percentile features\n",
    "feature_dict['percentile_10'] = np.percentile(x, 10)\n",
    "feature_dict['percentile_60'] = np.percentile(x, 60)\n",
    "feature_dict['percentile_90'] = np.percentile(x, 90)\n",
    "\n",
    "# quantile features\n",
    "feature_dict['quantile_5'] = np.quantile(x, 0.05)\n",
    "feature_dict['quantile_95'] = np.quantile(x, 0.95)\n",
    "feature_dict['quantile_99'] = np.quantile(x, 0.99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "diff requires input that is at least one dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4604\\2013826770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count_above_mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_above_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count_below_mean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_below_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfeature_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_abs_change'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_abs_change\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_change'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_change\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Basit Akram\\anaconda3\\lib\\site-packages\\tsfresh\\feature_extraction\\feature_calculators.py\u001b[0m in \u001b[0;36mmean_abs_change\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \"\"\"\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Basit Akram\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[0;32m   1256\u001b[0m     \u001b[0mnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diff requires input that is at least one dimensional\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1259\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: diff requires input that is at least one dimensional"
     ]
    }
   ],
   "source": [
    "from tsfresh.feature_extraction import feature_calculators as fc\n",
    "# tsfresh based features \n",
    "feature_dict['abs_energy'] = fc.abs_energy(x)\n",
    "feature_dict['count_above_mean'] = fc.count_above_mean(x)\n",
    "feature_dict['count_below_mean'] = fc.count_below_mean(x)\n",
    "feature_dict['mean_abs_change'] = fc.mean_abs_change(x)\n",
    "feature_dict['mean_change'] = fc.mean_change(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A random dataframe with two numerical features\n",
    "import numpy as np\n",
    "\n",
    "# generate a random dataframe with\n",
    "# 2 columns and 100 rows\n",
    "df = pd.DataFrame(\n",
    "    np.random.rand(100,2),\n",
    "    columns=[f\"f_{i}\" for i in range(1,3)]\n",
    ")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample dataframe with polynomial features\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# initialize polynomial features class object\n",
    "# for two-degree polynomial features\n",
    "pf = preprocessing.PolynomialFeatures(\n",
    "    degree=2,\n",
    "    interaction_only=False,\n",
    "    include_bias=False\n",
    ")\n",
    "\n",
    "# fit to the features\n",
    "pf.fit(df)\n",
    "\n",
    "# create polynomial features\n",
    "poly_feats = pf.transform(df)\n",
    "\n",
    "# create a dataframe with all the features\n",
    "num_feats = poly_feats.shape[1]\n",
    "df_transformed = pd.DataFrame(\n",
    "    poly_feats,\n",
    "    columns=[f\"f_{i}\" for i in range(1, num_feats + 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Basit Akram\\AppData\\Local\\Temp\\ipykernel_4604\\2130912506.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
      "C:\\Users\\Basit Akram\\AppData\\Local\\Temp\\ipykernel_4604\\2130912506.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"f_bin_100\"] = pd.cut(df[\"f_1\"],bins=100, labels=False)\n"
     ]
    }
   ],
   "source": [
    "# create bins of the numerical columns\n",
    "# 10 bins\n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
    "# 100 bins\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"],bins=100, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.282115\n",
       "1    0.473067\n",
       "2    0.191579\n",
       "3    0.533392\n",
       "4    0.863071\n",
       "Name: f_3, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"f_3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06783568766142445"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.f_3.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029968843877698933"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.f_3.apply(lambda x: np.log(1+x)).var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNImputer for handling missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7. 12.  6. 13. 14.  7.]\n",
      " [13. 12.  4.  3.  7. 13.]\n",
      " [ 7.  1. 12.  5.  7.  7.]\n",
      " [ 2.  1.  1.  9.  5.  2.]\n",
      " [14.  1.  4.  1.  7.  8.]\n",
      " [12. 12.  1.  1.  7.  2.]\n",
      " [12.  9. 14. 11. 13. 12.]\n",
      " [ 7. 13.  3.  3.  5. 11.]\n",
      " [ 6.  5.  1.  7.  3.  4.]\n",
      " [10.  8.  5.  1.  7.  7.]] \n",
      "\n",
      "[[ 7. 12.  6. 13. 14.  7.]\n",
      " [13. 12.  4.  3.  7. 13.]\n",
      " [ 7. nan 12.  5.  7. nan]\n",
      " [nan  1. nan  9. nan  2.]\n",
      " [14.  1. nan nan  7.  8.]\n",
      " [12. nan  1.  1.  7.  2.]\n",
      " [12.  9. 14. 11. 13. 12.]\n",
      " [ 7. 13.  3.  3.  5. 11.]\n",
      " [nan nan  1.  7.  3.  4.]\n",
      " [10.  8.  5.  1.  7.  7.]] \n",
      "\n",
      "[[ 7.  12.   6.  13.  14.   7. ]\n",
      " [13.  12.   4.   3.   7.  13. ]\n",
      " [ 7.   4.5 12.   5.   7.   4.5]\n",
      " [10.5  1.   6.5  9.   5.   2. ]\n",
      " [14.   1.   1.   4.   7.   8. ]\n",
      " [12.   4.5  1.   1.   7.   2. ]\n",
      " [12.   9.  14.  11.  13.  12. ]\n",
      " [ 7.  13.   3.   3.   5.  11. ]\n",
      " [13.   1.   1.   7.   3.   4. ]\n",
      " [10.   8.   5.   1.   7.   7. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import impute\n",
    "\n",
    "# create a random numpy array with 10 samples\n",
    "# and 6 features and values ranging from 1 to 15\n",
    "X = np.random.randint(1, 15, (10,6))\n",
    "\n",
    "# convert the array to float\n",
    "X = X.astype(float)\n",
    "\n",
    "print(X, \"\\n\")\n",
    "\n",
    "# randomly assign 10 elements to NaN (missing)\n",
    "X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n",
    "\n",
    "print(X,\"\\n\")\n",
    "# use 2 nearest neighbours to fill na values\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=2)\n",
    "\n",
    "X = knn_imputer.fit_transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  6. ,  2. ,  8. , 12. ,  6.5],\n",
       "       [12. , 11. ,  6. , 13. ,  8.5,  7. ],\n",
       "       [ 7. ,  7.5, 14. ,  7. ,  6. ,  3. ],\n",
       "       [13. ,  1. ,  8. ,  2. ,  5. , 14. ],\n",
       "       [ 3. ,  7.5,  9. ,  4. ,  3. , 10.5],\n",
       "       [14. ,  5. ,  1. ,  8. ,  6. ,  7. ],\n",
       "       [ 5. , 10. , 10. ,  8. ,  6. , 12. ],\n",
       "       [ 5. , 10.5, 11. ,  9. , 11. ,  7.5],\n",
       "       [ 5. ,  5. , 14. ,  3. ,  1. ,  9. ],\n",
       "       [12. ,  2. ,  1. ,  8. ,  7. ,  1. ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
